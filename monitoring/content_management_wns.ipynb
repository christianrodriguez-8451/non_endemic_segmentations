{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de1bf067-d5ae-4aa9-bc93-6ae60756d7ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### ATTENTION - Restatement of flow and code is under progress. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff682edc-751a-4032-b347-10dd6d4cbcff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####### little massy right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "531aedd5-ce00-4a04-b7d9-a946f22c1936",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from poirot.azure.databricks import svc_prncpl_magic_setup\n",
    "svc_prncpl_magic_setup(scope=\"kv-8451-tm-media-dev\", app_id_secret=\"spTmMediaDev-app-id\", app_pw_secret=\"spTmMediaDev-pw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb980a29-85b0-4d82-8b62-b6a63c473d62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Define service principals\n",
    "service_credential = dbutils.secrets.get(scope='kv-8451-tm-media-dev',key='spTmMediaDev-pw')\n",
    "service_application_id = dbutils.secrets.get(scope='kv-8451-tm-media-dev',key='spTmMediaDev-app-id')\n",
    "directory_id = \"5f9dc6bd-f38a-454a-864c-c803691193c5\"\n",
    "storage_account = 'sa8451dbxadhocprd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8525b7a-d258-42bc-bff9-3f0c288d6e6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", service_application_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ccd778e-6226-4b61-9d37-0b0e7dccd743",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Importing All the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kayday as kd\n",
    "import seg\n",
    "import toolbox.config as con\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from effodata import ACDS, golden_rules, Sifter, Equality, Joiner\n",
    "from toolbox.config import segmentation\n",
    "from kpi_metrics import KPI, AliasMetric, CustomMetric, AliasGroupby, available_metrics, get_metrics\n",
    "from seg.utils import DateType\n",
    "from flowcate.files import FilePath\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.dbutils import DBUtils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d58ec7-9d45-41de-94ba-d41dd4c6d4b4",
     "showTitle": true,
     "title": "Paths"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "timestamp = \"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/audience_factory/egress_receipts/\"\n",
    "eligibility_path = ('abfss://landingzone@sa8451entlakegrnprd.dfs.core.windows.net/mart/comms/prd/fact/eligibility_fact')\n",
    "files_path = dbutils.fs.ls(eligibility_path)\n",
    "\n",
    "sorted_files = sorted(files_path, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the second latest file\n",
    "second_latest_file = sorted_files[1]\n",
    "\n",
    "print(\"The second latest file is: \", second_latest_file.path)\n",
    "\n",
    "latest_eligibility_df = spark.read.parquet(eligibility_path) #This will give us the Dates we are using in. #Getting Eligiblity Table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b74136d0-58d7-42c0-88c9-ad0c91037f09",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Getting all the Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d514d0-dd4f-43ba-a9ff-0182e43b0f58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_all_segmentation_name():\n",
    "    '''\n",
    "    Get all the Segmentation names\n",
    "    \n",
    "    '''\n",
    "    segmentation_names = con.segmentations.all_segmentations\n",
    "\n",
    "    return segmentation_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37eb7bce-264f-4bc5-be24-68474163bb43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Combining All Segmentations Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64bb8f92-4f12-4aca-aaef-0497231e1444",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.dbutils import DBUtils\n",
    "\n",
    "dbutils = DBUtils()\n",
    "dir_contents = dbutils.fs.ls(timestamp)\n",
    "dir_contents = [ x[0] for x in dir_contents]\n",
    "segmentation_name = get_all_segmentation_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29916ffc-26d9-4edc-96bc-8164ed5ef049",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to check if any column contains 'Y'\n",
    "def any_column_contains_Y(*args):\n",
    "    return 'Y' in args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9100d5b5-485f-48f4-9cb9-55b2c243229a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_schema = t.StructType([\n",
    "    t.StructField(\"EHHN\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENT\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENTATION\", t.StringType(), True),\n",
    "    t.StructField(\"FRONTEND_NAME\", t.StringType(), True),\n",
    "    t.StructField(\"PROPENSITY\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENT_TYPE\", t.StringType(), True),\n",
    "    t.StructField(\"PERCENTILE_SEGMENT\", t.StringType(), True),\n",
    "    t.StructField(\"PERIOD\", t.StringType(), True),\n",
    "    t.StructField(\"TAGS\", t.StringType(), True)\n",
    "])\n",
    "df = spark.createDataFrame([], schema=my_schema)\n",
    "my_schema_elig = t.StructType([\n",
    "    t.StructField(\"EHHN\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENT\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENTATION\", t.StringType(), True),\n",
    "    t.StructField(\"FRONTEND_NAME\", t.StringType(), True),\n",
    "    t.StructField(\"PROPENSITY\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENT_TYPE\", t.StringType(), True),\n",
    "    t.StructField(\"PERCENTILE_SEGMENT\", t.StringType(), True),\n",
    "    t.StructField(\"PERIOD\", t.StringType(), True),\n",
    "    t.StructField(\"TAGS\", t.StringType(), True),\n",
    "    t.StructField(\"ONSITE_COUNT\", t.IntegerType(), True),\n",
    "    t.StructField(\"OFFSITE_COUNT\", t.IntegerType(), True),\n",
    "    t.StructField(\"OVERALL_ELIGIBILITY\", t.BooleanType(), True),\n",
    "])\n",
    "combined_segmentation_df = spark.createDataFrame([], schema=my_schema_elig)\n",
    "leftover_segment = {}\n",
    "for i in dir_contents:\n",
    "    for segmentations in segmentation_name:\n",
    "          try:\n",
    "            segment = con.segmentation(segmentations)\n",
    "            frontend_name = segment.frontend_name\n",
    "            propensities = segment.propensities\n",
    "            final_propensity = \"\".join(propensities)\n",
    "            tag = segment.tags\n",
    "            final_tag = \"\".join(tag)\n",
    "            type_of_segment = segment.segment_type\n",
    "            percentile_of_segment = segment.type\n",
    "            latest_file = i\n",
    "            rec = spark.read.csv(latest_file, header=True)\n",
    "            path = rec.filter(f.col('audience')== segmentations).select('filepath').distinct().collect()[0][0]\n",
    "            period = latest_file[-12:-4]\n",
    "            segment_file = spark.read.format(\"delta\").load(path)\n",
    "\n",
    "            segment_file = (\n",
    "                  segment_file.withColumn(\"SEGMENTATION\", f.lit(segmentations))\n",
    "                  .withColumn('FRONTEND_NAME', f.lit(frontend_name))\n",
    "                  .withColumn(\"PROPENSITY\", f.lit(final_propensity))\n",
    "                  .withColumn(\"SEGMENT_TYPE\", f.lit(type_of_segment))\n",
    "                  .withColumn(\"PERCENTILE_SEGMENT\", f.lit(percentile_of_segment))\n",
    "                  .withColumn(\"PERIOD\", f.lit(period))\n",
    "                  .withColumn(\"TAGS\", f.lit(final_tag))\n",
    "            \n",
    "            )\n",
    "            \n",
    "            segment_file = segment_file.filter(f.col(\"SEGMENT\").isin(segment.propensities))\n",
    "            segment_file = segment_file.select(\"EHHN\", \"SEGMENT\", \"SEGMENTATION\", \"FRONTEND_NAME\",\n",
    "                                                \"PROPENSITY\", \"SEGMENT_TYPE\", \"PERCENTILE_SEGMENT\",\"PERIOD\",\"TAGS\")\n",
    "            df = df.union(segment_file)\n",
    "            eligibility_analytical_dataset = df.join(latest_eligibility_df, on=\"ehhn\", how='left')\n",
    "\n",
    "            ### overall logic:\n",
    "\n",
    "            column_names = ['NATIVE_ELIGIBLE_FLAG','TDC_ELIGIBLE_FLAG','SSE_ELIGIBLE_FLAG','FFD_ELIGIBLE_FLAG','EMAIL_ELIGIBLE_FLAG','PUSH_FLAG',\n",
    "                            'FACEBOOK_FLAG','PANDORA_FLAG','CHICORY_FLAG','PUSH_FLAG','PREROLL_VIDEO_ELIGIBLE_FLAG','PINTEREST_ELIGIBLE_FLAG','ROKU_FLAG']\n",
    "\n",
    "            # Register UDF\n",
    "            contains_Y_udf = udf(any_column_contains_Y, BooleanType())\n",
    "\n",
    "            # Apply UDF to create a new column\n",
    "\n",
    "\n",
    "            eligibility_with_on_off_overall_flag = eligibility_analytical_dataset.withColumn('ONSITE_COUNT', f.when(((f.col('TDC_ELIGIBLE_FLAG') == 'Y') | (f.col('SSE_ELIGIBLE_FLAG') == 'Y') | (f.col('FFD_ELIGIBLE_FLAG') == 'Y') | (f.col('SS_ELIGIBLE_FLAG') == 'Y') | (f.col('PUSH_FLAG') == 'Y')), '1').otherwise('0').cast('integer'))\\\n",
    "                            .withColumn('OFFSITE_COUNT', f.when(((f.col('FACEBOOK_FLAG') == 'Y') | (f.col('LIVERAMP_FLAG') == 'Y') | (f.col('PANDORA_FLAG') == 'Y') | (f.col('PINTEREST_ELIGIBLE_FLAG') =='Y') | (f.col('PREROLL_VIDEO_ELIGIBLE_FLAG') == 'Y') | (f.col('CBA_ELIGIBLE_FLAG') == 'Y')), '1').otherwise('0').cast('integer'))\\\n",
    "                            .withColumn(\"OVERALL_ELIGIBILITY\", contains_Y_udf(*[col(column) for column in column_names]))\n",
    "\n",
    "            secondary_page_table = (eligibility_with_on_off_overall_flag.select(\"EHHN\",\"SEGMENT\",\"SEGMENTATION\",\"FRONTEND_NAME\",\"PROPENSITY\",\"SEGMENT_TYPE\",\"PERCENTILE_SEGMENT\",\"PERIOD\",\"TAGS\",\"ONSITE_COUNT\",\"OFFSITE_COUNT\",\"OVERALL_ELIGIBILITY\"))\n",
    "                            \n",
    "          \n",
    "          except Exception as e:\n",
    "                leftover_segment[segmentations] = (str(e) + str(dir_contents))\n",
    "                pass\n",
    "\n",
    "    combined_segmentation_df = combined_segmentation_df.union(secondary_page_table) #Get all segments combined\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c7ff52-b941-4c2d-88d5-46d1d8e6d084",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "leftover_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17e6d1a8-4cee-43a8-ad03-16272129d834",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_segmentation_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68351f17-2716-4f62-91db-ded8f31ef853",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print(combined_segmentation_df.show(25, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "143cc319-c559-4f2c-81fc-f465470a7ae4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "segmentation_count_df = (combined_segmentation_df.groupBy('SEGMENTATION', 'PERIOD')\n",
    "                        .agg(f.count('EHHN').alias(\"TOTAL_HH\"),\n",
    "                        f.max(\"frontend_name\").alias(\"FRONTEND_NAME\"),\n",
    "                        f.max(\"SEGMENT\").alias(\"SEGMENT\"),\n",
    "                        f.max(\"TAGS\").alias(\"TAGS\"),\n",
    "                        f.max(\"PROPENSITY\").alias(\"PROPENSITY\"),\n",
    "                        f.max(\"SEGMENT_TYPE\").alias(\"SEGMENT_TYPE\"),\n",
    "                        f.max(\"PERCENTILE_SEGMENT\").alias(\"PERCENTILE_SEGMENT\"),\n",
    "                        )\n",
    "                        .select(\"SEGMENTATION\",\"PERIOD\",\"TOTAL_HH\",\"FRONTEND_NAME\",\"SEGMENT\",\"TAGS\",\"PROPENSITY\",\"SEGMENT_TYPE\",\"PERCENTILE_SEGMENT\")\n",
    "                        \n",
    "                  )\n",
    "                  \n",
    "segmentation_onsite_offsite_count = (combined_segmentation_df.groupBy('SEGMENTATION', 'PERIOD')\n",
    "                        .agg(f.count('EHHN').alias(\"TOTAL_HH\"),\n",
    "                        f.max(\"frontend_name\").alias(\"FRONTEND_NAME\"),\n",
    "                        f.max(\"SEGMENT\").alias(\"SEGMENT\"),\n",
    "                        f.max(\"PROPENSITY\").alias(\"PROPENSITY\"),\n",
    "                        f.max(\"SEGMENT_TYPE\").alias(\"SEGMENT_TYPE\"),\n",
    "                        f.max(\"PERCENTILE_SEGMENT\").alias(\"PERCENTILE_SEGMENT\"),\n",
    "                        f.sum('ONSITE_COUNT').alias('ONSITE_COUNTS'),\n",
    "                        f.sum('OFFSITE_COUNT').alias('OFFSITE_COUNTS'),\n",
    "                        f.max(\"TAGS\").alias(\"TAGS\")\n",
    "                        )\n",
    "                        .select(\"SEGMENTATION\", \"PERIOD\", \"TOTAL_HH\", \"FRONTEND_NAME\", \"SEGMENT\", \"PROPENSITY\", \"SEGMENT_TYPE\", \"PERCENTILE_SEGMENT\", \"ONSITE_COUNTS\", \"OFFSITE_COUNTS\", \"TAGS\")\n",
    "                        \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9abd336-9e19-4c9d-ada9-7a64e7a7e251",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "segmentation_count_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/audience_factory/powerbi_inputs/household_counts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36fd5d94-23ee-4ee6-9fde-5bd58bcbeeb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "segmentation_onsite_offsite_count.coalesce(1).write.mode(\"overwrite\").parquet(f\"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/audience_factory/powerbi_inputs/eligibility_counts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c335700-74a8-45a6-adcf-159896733165",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window\n",
    "import commodity_segmentations.config as con\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import resources.config as config\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def get_image_size(url):\n",
    "  \"\"\" Returns size of the image located at the inputted URL.\n",
    "\n",
    "  Example\n",
    "  ----------\n",
    "    get_image_size('https://www.kroger.com/product/images/medium/front/0019264047717')\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  url: str\n",
    "    URL string of where the image is located.\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  A two-dimension tuple with the size image or None if there is no\n",
    "  image at the inputted URL.\n",
    "  \"\"\"\n",
    "  try:\n",
    "      # Download the image from the URL\n",
    "      response = requests.get(url)\n",
    "      response.raise_for_status()  # Raise an exception for 4xx and 5xx status codes\n",
    "\n",
    "      # Open the image using Pillow\n",
    "      image_data = io.BytesIO(response.content)\n",
    "      image = Image.open(image_data)\n",
    "\n",
    "      # Get the size of the image\n",
    "      width, height = image.size\n",
    "\n",
    "      return (width, height)\n",
    "  except Exception as e:\n",
    "      print(\"Error:\", e)\n",
    "      return None\n",
    "\n",
    "def draw_nonempty_upcs(df, n=25, upc_colname = 'gtin_no'):\n",
    "  \"\"\"Randomly draws UPCs and checks if they have an image\n",
    "  at https://www.kroger.com/product/images/medium/front/.\n",
    "  Re-draws the UPC if there is no image found for that UPC.\n",
    "\n",
    "  Example\n",
    "  ----------\n",
    "    draw_nonempty_upcs(df, 25, 'gtin_no')\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  df: pyspark.sql.dataframe.DataFrame\n",
    "    PySpark dataframe that has a column with UPC codes.\n",
    "\n",
    "  n: int\n",
    "    Integer that defines how many UPCs to draw.\n",
    "\n",
    "  upc_colname: str\n",
    "    String that defines the column name in df that contains the\n",
    "    UPC values. \n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  A list that contains the randomly drawn UPC codes that are confirmed\n",
    "  to have an image at https://www.kroger.com/product/images/medium/front/.\n",
    "  \"\"\"\n",
    "  #We do a single collect of 50000 randomly drawn UPCs\n",
    "  #store in a list and cut down on processing time during the re-draws.\n",
    "  upc_bank = df.\\\n",
    "    select(upc_colname).\\\n",
    "    sample(False, 0.1).\\\n",
    "    limit(50000).\\\n",
    "    rdd.map(lambda x: x[0]).\\\n",
    "    collect()\n",
    "\n",
    "  #Initially draw of UPCs  \n",
    "  random_upcs = random.sample(upc_bank, n)\n",
    "  \n",
    "  url_fmt = \"https://www.kroger.com/product/images/medium/front/{}\"\n",
    "  bad_upcs = []\n",
    "  good_upcs = []\n",
    "  dont_redraw = []\n",
    "  redrawn_upcs = random_upcs\n",
    "  while len(redrawn_upcs) != 0:\n",
    "    #For each UPC, check if they have no image\n",
    "    for upc in redrawn_upcs:\n",
    "      url = url_fmt.format(upc)\n",
    "      size = get_image_size(url)\n",
    "\n",
    "      #If there is no image, then add them to bad UPC list for re-draw\n",
    "      if size is None:\n",
    "        print(\"{} is a bad UPC!\".format(upc))\n",
    "        bad_upcs += [upc]\n",
    "        dont_redraw += [upc]\n",
    "\n",
    "      #Is there is an image, then add them to good UPC list for the output\n",
    "      else:\n",
    "        good_upcs += [upc]\n",
    "        dont_redraw += [upc]\n",
    "\n",
    "    #Drop the UPCs that do not have an image and re-draw from the dataframe\n",
    "    if len(bad_upcs) != 0:\n",
    "      print(\"Executing re-draw...\")\n",
    "\n",
    "      #Filter out the UPCs we have already drawn\n",
    "      [upc_bank.remove(d) for d in dont_redraw]\n",
    "      #Define n that we have to re-draw\n",
    "      redraw_n = len(bad_upcs)\n",
    "      #Reset bad UPC list for the next iteration\n",
    "      bad_upcs = []\n",
    "      #Reset the don't-redraw deck since we already dropped those from the bank\n",
    "      dont_redraw = []\n",
    "\n",
    "      #Do the re-draw\n",
    "      if len(upc_bank) != 0:\n",
    "        redrawn_upcs = random.sample(upc_bank, redraw_n)\n",
    "\n",
    "      #Ran out of UPCs. There is nothing to re-draw from.\n",
    "      else:\n",
    "        redrawn_upcs = []\n",
    "        print(\"Could not redraw any more UPCs. Ran out of UPCs.\")\n",
    "    \n",
    "    #There is no bad UPCs to re-draw\n",
    "    else:\n",
    "      redrawn_upcs = []\n",
    "\n",
    "  #We tried re-drawing as much as possible - no UPC had an image to show\n",
    "  if len(good_upcs) == 0:\n",
    "    raise ValueError(\"No UPC in the list has an image at www.kroger.com.\")\n",
    "  #We tried re-drawing, but could only get so many images\n",
    "  elif len(good_upcs) > 0 and len(good_upcs) < n:\n",
    "    print(\"Only {} UPCs had an image at www.kroger.com.\")\n",
    "  \n",
    "  upc_with_url = []\n",
    "  for i in good_upcs:\n",
    "    upc = url_fmt.format(i)\n",
    "    upc_with_url.append(upc)\n",
    "\n",
    "  return(good_upcs, upc_with_url)\n",
    "\n",
    "def display_upcs(upcs):\n",
    "  \"\"\"Returns an image from https://www.kroger.com/product/images/medium/front/\n",
    "  for each UPC in the inputted list.\n",
    "\n",
    "  Example\n",
    "  ----------\n",
    "    display_upcs(['0082778285046', '0007248921915', '0081235026102'])\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  upcs: list\n",
    "    List of strings where each string is a UPC code. UPC codes\n",
    "    need to be string since leading zeroes matter.\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  A pandas.io.formats.style.Styler object that has an image\n",
    "  for each inputted UPC.\n",
    "  \"\"\"\n",
    "  ldf=pd.DataFrame(upcs,columns=['UPC'])\n",
    "  ldf = ldf.sort_values(by='UPC', ascending=False)\n",
    "  ldf = ldf.reset_index(drop=True)\n",
    "  ldf['IMG']=ldf['UPC'].map(lambda x: f'<img src=\"https://www.kroger.com/product/images/medium/front/{x}\">' )#if x.isnumeric() else '')\n",
    "  return ldf.T.style\n",
    "\n",
    "def top_words(df, string_col, n=100):\n",
    "  \"\"\"Returns the top n words for the inputted column\n",
    "  in df.\n",
    "\n",
    "  Example\n",
    "  ----------\n",
    "    top_words(df, 'product_name', 100)\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  df: pyspark.sql.dataframe.DataFrame\n",
    "    PySpark dataframe that has the string column we'd\n",
    "    like to extract the top n words from.\n",
    "\n",
    "  string_col: str\n",
    "    String that the defines the column name of the \n",
    "    column of interest. \n",
    "\n",
    "  n: int\n",
    "    Integer that defines the top n cut-off.\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  A list where the first element is the most frequent word and the\n",
    "  last element is the n-th most frequent word. Each element is formatted\n",
    "  as '{word} ({count})' where the parentheses contains the count\n",
    "  of how many times that word occured when considering all values \n",
    "  in the given column. \n",
    "  \"\"\"\n",
    "  #Get the complete word occurence in each commodity/sub-commodity\n",
    "  word_count = df.select(f.explode(f.split(df[string_col], \" \")).alias(\"word\"))\n",
    "  word_count = word_count.withColumn(\"word\", f.regexp_replace(\"word\", \",\", \"\"))\n",
    "  word_count = word_count.withColumn(\"length\", f.length(\"word\"))\n",
    "  word_count = word_count.filter(f.col(\"length\") > 0)\n",
    "  word_count = word_count.groupBy([\"word\"]).count()\n",
    "  windowSpec = Window.orderBy(f.desc(\"count\"))\n",
    "  word_count = word_count.withColumn(\"row_index\", f.row_number().over(windowSpec))\n",
    "\n",
    "  #Collect the top n words and output them as a list since PySpark only\n",
    "  #shows a maximum of 50 rows\n",
    "  word_count = word_count.filter(f.col(\"row_index\") <= n)\n",
    "  words = word_count.select(\"word\").collect()\n",
    "  words = [row[0] for row in words]\n",
    "  counts = word_count.select(\"count\").collect()\n",
    "  counts = [row[0] for row in counts]\n",
    "  top_words = [\"{} ({})\".format(w, c) for w, c in zip(words, counts)]\n",
    "  \n",
    "  return(top_words)\n",
    "\n",
    "def display_counts(df, col, limit=15):\n",
    "  \"\"\"Returns the grouped-by counts for the inputted\n",
    "  column and visualizes the counts via a bar chart.\n",
    "\n",
    "  Example\n",
    "  ----------\n",
    "    display_counts(df, \"department\", 15)\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  df: pyspark.sql.dataframe.DataFrame\n",
    "    PySpark dataframe contains the data we'd like\n",
    "    to conduct the group-by count on.\n",
    "\n",
    "  col: str\n",
    "    String that the defines the column name of the \n",
    "    column of interest. \n",
    "\n",
    "  limit: int\n",
    "    Integer that defines the the cut-off of how many\n",
    "    bars to plot in the bar chart. Reduces cluttering.\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  None. Counts and visual are displayed as cell output.\n",
    "  \"\"\"\n",
    "  #Get counts - organize from most dominant to least\n",
    "  counts_df = df.\\\n",
    "    groupBy(col).\\\n",
    "    count().\\\n",
    "    orderBy('count', ascending=False)\n",
    "  #Display counts of top 50\n",
    "  counts_df.show(50, truncate=False)\n",
    "\n",
    "  #Limit the bar plot to top 15 to avoid overcluttering\n",
    "  windowSpec = Window.orderBy(f.desc(\"count\"))\n",
    "  counts_df = counts_df.withColumn(\"row_index\", f.row_number().over(windowSpec))\n",
    "  counts_df = counts_df.filter(f.col(\"row_index\") <= limit)\n",
    "\n",
    "  # Convert Spark DataFrame to Pandas DataFrame for plotting\n",
    "  counts_df = counts_df.toPandas()\n",
    "\n",
    "  # Plotting\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.bar(counts_df[col], counts_df['count'])\n",
    "  plt.xlabel(col)\n",
    "  plt.ylabel('Counts')\n",
    "  plt.title('Counts by {}'.format(col))\n",
    "  plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "  plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
    "  plt.show()\n",
    "\n",
    "def display_histplot(df, col):\n",
    "  \"\"\"Visualizes a histogram for the inputted\n",
    "  column in the given dataframe.\n",
    "\n",
    "  Example\n",
    "  ----------\n",
    "    display_histplot(df, \"dot_product\")\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  df: pyspark.sql.dataframe.DataFrame\n",
    "    PySpark dataframe contains the data we'd like\n",
    "    to conduct the group-by count on.\n",
    "\n",
    "  col: str\n",
    "    String that the defines the column name of the \n",
    "    column of interest.\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  None. Visual is displayed as cell output.\n",
    "  \"\"\"\n",
    "\n",
    "  pandas_df = df.toPandas()\n",
    "\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  sns.histplot(pandas_df[col], kde=True)\n",
    "  plt.title(\"Distribution of {}\".format(col))\n",
    "  plt.xlabel(\"Values\")\n",
    "  plt.ylabel(\"Frequency\")\n",
    "  plt.show()\n",
    "\n",
    "def write_out(df, fp, delim=\",\", fmt=\"csv\"):\n",
    "  \"\"\"Writes out PySpark dataframe as a csv file\n",
    "  that can be downloaded for Azure and loaded into\n",
    "  Excel very easily.\n",
    "\n",
    "  Example\n",
    "  ----------\n",
    "    write_out(df, \"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/audience_factory/adhoc/data_analysis.csv\")\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  df: pyspark.sql.dataframe.DataFrame\n",
    "    PySpark dataframe contains the data we'd like\n",
    "    to conduct the group-by count on.\n",
    "\n",
    "  fp: str\n",
    "    String that the defines the column name of the \n",
    "    column of interest.\n",
    "\n",
    "  delim: str\n",
    "    String that specifies which delimiter to use in the\n",
    "    write-out. Default value is ','.\n",
    "\n",
    "  fmt: str\n",
    "    String that specifies which format to use in the\n",
    "    write-out. Default value is 'csv'.\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  None. File is written out specified Azure location.\n",
    "  \"\"\"\n",
    "  #Placeholder filepath for intermediate processing\n",
    "  temp_target = os.path.dirname(fp) + \"/\" + \"temp\"\n",
    "\n",
    "  #Write out df as partitioned file. Write out ^ delimited\n",
    "  df.coalesce(1).write.options(header=True, delimiter=delim).mode(\"overwrite\").format(fmt).save(temp_target)\n",
    "\n",
    "  #Copy desired file from parititioned directory to desired location\n",
    "  temporary_fp = os.path.join(temp_target, dbutils.fs.ls(temp_target)[3][1])\n",
    "  dbutils.fs.cp(temporary_fp, fp)\n",
    "  dbutils.fs.rm(temp_target, recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cad9921-b7a3-4194-9658-0c04423afbcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from functools import reduce\n",
    "from effodata import ACDS, golden_rules, Joiner, Sifter, Equality, join_on\n",
    "from kpi_metrics import KPI, AliasMetric, CustomMetric, AliasGroupby\n",
    "from IPython.display import FileLink \n",
    "import sys\n",
    "from typing import Union\n",
    "import datetime as dt\n",
    "from datetime import timedelta, datetime, date\n",
    "\n",
    "segmentation_list = segmentation_name\n",
    "acds = ACDS(use_sample_mart = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c939016-486e-4411-b94b-058fcfca91de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "srt_dt = (date.today() - timedelta(days=385)).strftime(\"%Y%m%d\")\n",
    "end_dt = (date.today() - timedelta(days=10)).strftime(\"%Y%m%d\")\n",
    "srt_dt, end_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79a898fb-5dd6-4f65-81e3-ec77e20c51cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "my_schema_elig = t.StructType([\n",
    "    t.StructField(\"UPC\", t.StringType(), True),\n",
    "    t.StructField(\"URL\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENTATION\", t.StringType(), True),\n",
    "])\n",
    "final_df = spark.createDataFrame([], schema=my_schema_elig)\n",
    "failed_seg = {}\n",
    "\n",
    "for i in segmentation_list: \n",
    "    try: \n",
    "      seg_dir = f\"{con.output_fp}upc_lists/{i}/\"\n",
    "      dir_contents = dbutils.fs.ls(seg_dir)\n",
    "      dir_contents = [x[0] for x in dir_contents if i in x[1]]\n",
    "      dir_contents.sort()\n",
    "      seg_fp = dir_contents[-1]\n",
    "      df = spark.read.format(\"delta\").load(seg_fp)\n",
    "      transaction = acds.get_transactions(start_date=srt_dt, end_date=end_dt)\n",
    "      gtins = (transaction.groupBy('gtin_no')\n",
    "              .agg(f.count('transaction_code').alias('Total Redemers'),\n",
    "                  f.count('ehhn').alias(\"Total Households\"),\n",
    "                  f.countDistinct('ehhn').alias(\"Total Distinct Counts\"))\n",
    "              .sort('Total Redemers', ascending=False))\n",
    "      getting_joined = df.join(gtins, on='gtin_no', how='left').sort('Total Redemers', ascending=False).select('gtin_no')\n",
    "      n = 10\n",
    "      image_upcs, upc_url = draw_nonempty_upcs(getting_joined, n)\n",
    "\n",
    "      # Combine the lists using zip\n",
    "      combined_data = zip(image_upcs, upc_url)\n",
    "\n",
    "      # Create a DataFrame with column names\n",
    "      df = spark.createDataFrame(combined_data, [\"UPC\", \"URL\"])\n",
    "      upc_file = (df.withColumn(\"SEGMENTATION\", f.lit(i)))\n",
    "\n",
    "      final_df = final_df.union(upc_file)\n",
    "\n",
    "    except Exception as e:\n",
    "          failed_seg[i] = str(e)       \n",
    "      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d389074-a5d3-456c-bfba-e577211eb68c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.coalesce(1).write.mode(\"overwrite\").parquet(f\"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/audience_factory/powerbi_inputs/upc_via_segmentation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7294d0b1-c810-43e7-9e14-4fbd755cc0a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "failed_seg.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d28c6d6a-178e-4c42-9f78-e3be7e2a95e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "content_management_wns",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
