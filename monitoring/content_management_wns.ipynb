{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ccd778e-6226-4b61-9d37-0b0e7dccd743",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Importing All the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from datetime import datetime, timedelta\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from effodata import ACDS, golden_rules, Sifter, Equality, Joiner\n",
    "import kayday as kd\n",
    "from toolbox.config import segmentation\n",
    "from kpi_metrics import KPI, AliasMetric, CustomMetric, AliasGroupby, available_metrics, get_metrics\n",
    "import seg\n",
    "from seg.utils import DateType\n",
    "from flowcate.files import FilePath\n",
    "import toolbox.config as con\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "import datetime as dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d58ec7-9d45-41de-94ba-d41dd4c6d4b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Getting Eligiblity Table\n",
    "\n",
    "eligibility_path = FilePath('abfss://landingzone@sa8451entlakegrnprd.dfs.core.windows.net/mart/comms/prd/fact/eligibility_fact')\n",
    "latest_eligibility_df = spark.read.parquet(eligibility_path.find_latest_file()) #This will give us the Dates we are using in.\n",
    "print(eligibility_path.find_latest_file())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b74136d0-58d7-42c0-88c9-ad0c91037f09",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Getting all the Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d514d0-dd4f-43ba-a9ff-0182e43b0f58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_all_segmentation_name():\n",
    "    '''\n",
    "    Get all the Segmentation names\n",
    "    \n",
    "    '''\n",
    "    segmentation_names = con.segmentations.all_segmentations\n",
    "\n",
    "    return segmentation_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3adb74dc-97c0-4ba8-aff1-68abb0833ba5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_all_columns(segmentation_names):\n",
    "    '''\n",
    "    Input: Segmentation names\n",
    "    Output: Final Dataframe with all segmentation\n",
    "    '''\n",
    "\n",
    "    my_schema = t.StructType([\n",
    "        t.StructField(\"EHHN\", t.StringType(), True),\n",
    "        t.StructField(\"SEGMENT\", t.StringType(), True),\n",
    "        t.StructField(\"SEGMENTATION\", t.StringType(), True),\n",
    "        t.StructField(\"FRONTEND_NAME\", t.StringType(), True),\n",
    "        t.StructField(\"PROPENSITY\", t.StringType(), True),\n",
    "        t.StructField(\"SEGMENT_TYPE\", t.StringType(), True),\n",
    "        t.StructField(\"PERCENTILE_SEGMENT\", t.StringType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame([], schema=my_schema)\n",
    "    leftover_segment = []\n",
    "    for segmentations in segmentation_names:\n",
    "          try:\n",
    "            segment = con.segmentation(segmentations)\n",
    "            frontend_name = segment.frontend_name\n",
    "            propensities = segment.propensities\n",
    "            final_propensity = \"\".join(propensities)\n",
    "            type_of_segment = segment.segment_type\n",
    "            percentile_of_segment = segment.type\n",
    "            latest_file = segment.files[-1]\n",
    "            reading_file = segment.directory + latest_file\n",
    "            segment_file = spark.read.format(\"delta\").load(reading_file)\n",
    "\n",
    "            segment_file = (\n",
    "                  segment_file.withColumn(\"SEGMENTATION\", f.lit(segmentations))\n",
    "                  .withColumn('FRONTEND_NAME', f.lit(frontend_name))\n",
    "                  .withColumn(\"PROPENSITY\", f.lit(final_propensity))\n",
    "                  .withColumn(\"SEGMENT_TYPE\", f.lit(type_of_segment))\n",
    "                  .withColumn(\"PERCENTILE_SEGMENT\", f.lit(percentile_of_segment))\n",
    "            )\n",
    "            \n",
    "            segment_file = segment_file.filter(f.col(\"SEGMENT\").isin(segment.propensities))\n",
    "            segment_file = segment_file.select(\"EHHN\", \"SEGMENT\", \"SEGMENTATION\", \"FRONTEND_NAME\",\n",
    "                                                \"PROPENSITY\", \"SEGMENT_TYPE\", \"PERCENTILE_SEGMENT\")\n",
    "            df = df.union(segment_file)\n",
    "          \n",
    "          except Exception as e:\n",
    "                leftover_segment.append(str(e))\n",
    "                pass\n",
    "\n",
    "    return df, leftover_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e20e65c8-a7a5-4db8-af67-bf8b54016df4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "segmentation_name = get_all_segmentation_name()\n",
    "final_df, leftover_seg = get_all_columns(segmentation_names=segmentation_name)\n",
    "segmentation_name_df = spark.createDataFrame([row for row in [(value,) for value in segmentation_name]], [\"SEGMENTATION\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2985bf06-017a-4695-85e3-7830ecfed008",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#If the result is empty then we are getting all the segmentation if not then we are missing one.\n",
    "\n",
    "display(segmentation_name_df.join(final_df, on='SEGMENTATION', how='left_anti')\n",
    "        .select('segmentation').distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92fd72a3-12e2-419e-a46c-aa2fa54d78df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_df.sort('EHHN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efec4d8a-8a1b-4f36-86ad-7720aab888d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Frank's Edits\n",
    "### need to rename df, this is too generic for troubleshooting later and might overwrite things that were named df before;\n",
    "\n",
    "#Heather's Old Logic (2+ year old)\n",
    "# select divis,\n",
    "# kpm_two,\n",
    "# EM_ELIG,\n",
    "# ecommerce_flag,\n",
    "# count(distinct ehhn)\n",
    "# from (select a.*, case when tdc_eligible_flag = 'Y' or sse_eligible_flag = 'Y' or ffd_eligible_flag = 'Y' or email_eligible_flag ='Y' or cba_eligible_flag ='Y' or\n",
    "# mobile_eligible_flag='Y' or has_digital_account_flag='Y' or pinterest_eligible_flag ='Y' then 'Y' else 'N' end as KPM_ELIG,\n",
    "# case when sse_eligible_flag = 'Y' or ffd_eligible_flag = 'Y' then 'Y' else 'N' end as EM_ELIG,\n",
    "# case when preferred_store_division is null or preferred_store_division = 'DEFAULT_HEIRARCHY' then last_store_shop_divison else preferred_store_division end as divis,\n",
    "# case when facebook_flag ='Y' or ecommerce_flag='Y' or sse_eligible_flag = 'Y' or ffd_eligible_flag = 'Y' then 'Y' else 'N' end as kpm_two\n",
    "# from  MKT_CM.ELIGIBILITY_FACT a\n",
    "# where date_id = to_date(20220206,'YYYYMMDD')\n",
    "# and last_shop_date >= to_date(20200210,'YYYYMMDD'))\n",
    " \n",
    "# group by divis,\n",
    "# kpm_two,\n",
    "# EM_ELIG,\n",
    "# ecommerce_flag;\n",
    "# Heather's logic:\n",
    "\n",
    "## put these in the story:\n",
    "\n",
    "# Heather's Comments: 1) we need to X.98 for the result we get for the reason of LT holdout\n",
    "# we dont usually (we never?) target a shopper that has not been in a Kroger store in the past year - if last shoppeed date is outside of the the current year, we deem the customer as non-active/non-eligible (Frank cannot get to this)\n",
    "\n",
    "eligibility_analytical_dataset = final_df.join(latest_eligibility_df, on=\"ehhn\", how='left')\n",
    "### eligibility flags (onsite, offsite, overall)\n",
    "\n",
    "### overall logic:\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "# data = [(\"John\", \"Y\", \"N\", \"Y\"),\n",
    "#         (\"Alice\", \"N\", \"N\", \"N\"),\n",
    "#         (\"Bob\", \"Y\", \"Y\", \"Y\")]\n",
    "\n",
    "# # Create DataFrame\n",
    "# df = spark.createDataFrame(data, [\"Name\", \"Col1\", \"Col2\", \"Col3\"])\n",
    "\n",
    "column_names = ['NATIVE_ELIGIBLE_FLAG','TDC_ELIGIBLE_FLAG','SSE_ELIGIBLE_FLAG','FFD_ELIGIBLE_FLAG','EMAIL_ELIGIBLE_FLAG','PUSH_FLAG',\n",
    "                'FACEBOOK_FLAG','PANDORA_FLAG','CHICORY_FLAG','PUSH_FLAG','PREROLL_VIDEO_ELIGIBLE_FLAG','PINTEREST_ELIGIBLE_FLAG','ROKU_FLAG']\n",
    "\n",
    "# Function to check if any column contains 'Y'\n",
    "def any_column_contains_Y(*args):\n",
    "    return 'Y' in args\n",
    "\n",
    "# Register UDF\n",
    "contains_Y_udf = udf(any_column_contains_Y, BooleanType())\n",
    "\n",
    "# Apply UDF to create a new column\n",
    "\n",
    "\n",
    "eligibility_with_on_off_overall_flag = eligibility_analytical_dataset.withColumn('onsite_flag', f.when(((f.col('TDC_ELIGIBLE_FLAG') == 'Y') | (f.col('SSE_ELIGIBLE_FLAG') == 'Y') | (f.col('NATIVE_ELIGIBLE_FLAG') == 'Y')|(f.col('EMAIL_ELIGIBLE_FLAG') == 'Y') | (f.col('FFD_ELIGIBLE_FLAG') == 'Y') | (f.col('SS_ELIGIBLE_FLAG') == 'Y') | (f.col('PUSH_FLAG') == 'Y')), '1').otherwise('0').cast('integer'))\\\n",
    "                  .withColumn('offsite_flag', f.when(((f.col('FACEBOOK_FLAG') == 'Y') | (f.col('PANDORA_FLAG') == 'Y') | (f.col('PINTEREST_ELIGIBLE_FLAG') =='Y') | (f.col('PREROLL_VIDEO_ELIGIBLE_FLAG') == 'Y') | (f.col('CBA_ELIGIBLE_FLAG') == 'Y') | (f.col('ROKU_FLAG') == 'Y') | (f.col('CHICORY_FLAG') == 'Y')), '1').otherwise('0').cast('integer'))\\\n",
    "                  .withColumn(\"Overall Eligibility\", contains_Y_udf(*[col(column) for column in column_names]))\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b5085de-de14-42b1-b26e-be8f4f52baba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(eligibility_with_on_off_overall_flag.limit(5)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8507bf2-1808-46bd-9f93-29235e926b88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(eligibility_with_on_off_overall_flag.groupBy('SEGMENTATION')\n",
    "        .agg(f.count('EHHN').alias(\"TOTAL_HH\"),\n",
    "             f.max(\"frontend_name\").alias(\"FRONTEND_NAME\"),\n",
    "             f.max(\"SEGMENT\").alias(\"SEGMENT\"),\n",
    "             f.max(\"PROPENSITY\").alias(\"PROPENSITY\"),\n",
    "             f.max(\"SEGMENT_TYPE\").alias(\"SEGMENT_TYPE\"),\n",
    "             f.max(\"PERCENTILE_SEGMENT\").alias(\"PERCENTILE_SEGMENT\"),\n",
    "             f.sum('onsite_flag').alias('ONSITE_COUNT'),\n",
    "             f.sum('offsite_flag').alias('OFFSITE_COUNT')\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf410212-7e7d-4ece-b5d6-a0e5ad3152ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extra's for checking onsite and offsite flag\n",
    "\n",
    "# getting_count = (final_df.join(elig, on=\"ehhn\", how='left')\n",
    "#                   .withColumn('onsite_flag', f.when(((f.col('TDC_ELIGIBLE_FLAG') == 'Y') | (f.col('SSE_ELIGIBLE_FLAG') == 'Y') | (f.col('NATIVE_ELIGIBLE_FLAG') == 'Y') |(f.col('EMAIL_ELIGIBLE_FLAG') == 'Y') | (f.col('PUSH_FLAG') == 'Y')), '1').otherwise('0').cast('integer'))\n",
    "#                   .withColumn('offsite_flag', f.when(((f.col('FACEBOOK_FLAG') == 'Y') | (f.col('PANDORA_FLAG') == 'Y') | (f.col('PINTEREST_ELIGIBLE_FLAG') =='Y') | (f.col('PREROLL_VIDEO_ELIGIBLE_FLAG') == 'Y') | (f.col('ROKU_FLAG') == 'Y') | (f.col('CHICORY_FLAG') == 'Y')), '1').otherwise('0').cast('integer'))\n",
    "                  \n",
    "#                   )\n",
    "\n",
    "# ehhn_invalid = (getting_count\n",
    "#         .filter((f.col('onsite_flag') == '0') & (f.col('offsite_flag') == '0'))\n",
    "# )\n",
    "# times = []\n",
    "# for i in file:\n",
    "#   names = i[0:-1]\n",
    "#   times.append(names)\n",
    "# times\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "content_management_wns",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
