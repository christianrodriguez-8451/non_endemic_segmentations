{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de1bf067-d5ae-4aa9-bc93-6ae60756d7ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ATTENTION - Restatement of flow and code is under progress. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff682edc-751a-4032-b347-10dd6d4cbcff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####### little massy right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "945b4882-adb5-40c2-9f1f-c297ad502fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install 'protobuf<=3.20.1' --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "531aedd5-ce00-4a04-b7d9-a946f22c1936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from poirot.azure.databricks import svc_prncpl_magic_setup\n",
    "svc_prncpl_magic_setup(scope=\"kv-8451-tm-media-dev\", app_id_secret=\"spTmMediaDev-app-id\", app_pw_secret=\"spTmMediaDev-pw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb980a29-85b0-4d82-8b62-b6a63c473d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Define service principals\n",
    "service_credential = dbutils.secrets.get(scope='kv-8451-tm-media-dev',key='spTmMediaDev-pw')\n",
    "service_application_id = dbutils.secrets.get(scope='kv-8451-tm-media-dev',key='spTmMediaDev-app-id')\n",
    "directory_id = \"5f9dc6bd-f38a-454a-864c-c803691193c5\"\n",
    "storage_account = 'sa8451dbxadhocprd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8525b7a-d258-42bc-bff9-3f0c288d6e6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", service_application_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ccd778e-6226-4b61-9d37-0b0e7dccd743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Importing All the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kayday as kd\n",
    "import seg\n",
    "import toolbox.config as tox_con\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from effodata import ACDS, golden_rules, Sifter, Equality, Joiner\n",
    "from toolbox.config import segmentation\n",
    "from kpi_metrics import KPI, AliasMetric, CustomMetric, AliasGroupby, available_metrics, get_metrics\n",
    "from seg.utils import DateType\n",
    "from flowcate.files import FilePath\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.dbutils import DBUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25c8da36-3aec-4d86-8495-499e5487491f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.commitProtocolClass\", \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\")\n",
    "spark.conf.set('parquet.enable.summary-metadata', 'false')\n",
    "spark.conf.set('mapreduce.fileoutputcommitter.marksuccessfuljobs', 'false')\n",
    "spark.conf.set(\"spark.databricks.queryWatchdog.maxQueryTasks\", \"400000\")\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d58ec7-9d45-41de-94ba-d41dd4c6d4b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Paths"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "timestamp = \"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/audience_factory/egress_receipts/\"\n",
    "eligibility_path = ('abfss://landingzone@sa8451entlakegrnprd.dfs.core.windows.net/mart/comms/prd/fact/eligibility_fact')\n",
    "hh_location = \"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/audience_factory/powerbi_inputs/household_counts/\"\n",
    "elig_file = \"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/audience_factory/powerbi_inputs/eligibility_counts/\"\n",
    "image_location =  \"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/audience_factory/powerbi_inputs/valid_image_upcs/\"\n",
    "image_upc = \"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/audience_factory/powerbi_inputs/upc_via_segmentation/\"\n",
    "\n",
    "filtered_time_period_lst = spark.read.parquet(hh_location).select(\"PERIOD\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "final_per_list = ', '.join(map(str, filtered_time_period_lst))\n",
    "\n",
    "files_path = dbutils.fs.ls(eligibility_path)\n",
    "\n",
    "sorted_files = sorted(files_path, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the second latest file\n",
    "second_latest_file = sorted_files[1]\n",
    "\n",
    "print(\"The latest file is: \", second_latest_file.path)\n",
    "\n",
    "latest_eligibility_df = spark.read.parquet(second_latest_file.path) #This will give us the Dates we are using in. #Getting Eligiblity Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d514d0-dd4f-43ba-a9ff-0182e43b0f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_all_segmentation_name():\n",
    "    '''\n",
    "    Get all the Segmentation names\n",
    "    \n",
    "    '''\n",
    "    segmentation_names = tox_con.segmentations.all_segmentations\n",
    "\n",
    "    return segmentation_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64bb8f92-4f12-4aca-aaef-0497231e1444",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils = DBUtils()\n",
    "needs_to_measured = []\n",
    "dir_contents = dbutils.fs.ls(timestamp)\n",
    "dir_contents = [ x[0] for x in dir_contents]\n",
    "\n",
    "segmentation_name = get_all_segmentation_name()\n",
    "\n",
    "for i in range(0, len(dir_contents)):\n",
    "  if dir_contents[i][-12:-4] in final_per_list:\n",
    "    print(f'Period already exist {dir_contents[i][-12:-4]}')\n",
    "  else:\n",
    "    needs_to_measured.append(dir_contents[i])\n",
    "\n",
    "dir_contents = needs_to_measured \n",
    "print(f'getting new period {dir_contents}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29916ffc-26d9-4edc-96bc-8164ed5ef049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to check if any column contains 'Y'\n",
    "def any_column_contains_Y(*args):\n",
    "    return 'Y' in args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9100d5b5-485f-48f4-9cb9-55b2c243229a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_periodic_hh(latest_file):\n",
    "    my_schema = t.StructType([\n",
    "    t.StructField(\"EHHN\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENT\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENTATION\", t.StringType(), True),\n",
    "    t.StructField(\"FRONTEND_NAME\", t.StringType(), True),\n",
    "    t.StructField(\"PROPENSITY\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENT_TYPE\", t.StringType(), True),\n",
    "    t.StructField(\"PERCENTILE_SEGMENT\", t.StringType(), True),\n",
    "    t.StructField(\"PERIOD\", t.StringType(), True),\n",
    "        t.StructField(\"TAGS\", t.StringType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame([], schema=my_schema)\n",
    "    my_schema_elig = t.StructType([\n",
    "        t.StructField(\"EHHN\", t.StringType(), True),\n",
    "        t.StructField(\"SEGMENT\", t.StringType(), True),\n",
    "        t.StructField(\"SEGMENTATION\", t.StringType(), True),\n",
    "        t.StructField(\"FRONTEND_NAME\", t.StringType(), True),\n",
    "        t.StructField(\"PROPENSITY\", t.StringType(), True),\n",
    "        t.StructField(\"SEGMENT_TYPE\", t.StringType(), True),\n",
    "        t.StructField(\"PERCENTILE_SEGMENT\", t.StringType(), True),\n",
    "        t.StructField(\"PERIOD\", t.StringType(), True),\n",
    "        t.StructField(\"TAGS\", t.StringType(), True),\n",
    "        t.StructField(\"ONSITE_COUNT\", t.IntegerType(), True),\n",
    "        t.StructField(\"OFFSITE_COUNT\", t.IntegerType(), True),\n",
    "        t.StructField(\"OVERALL_ELIGIBILITY\", t.BooleanType(), True),\n",
    "    ])\n",
    "    combined_segmentation_df = spark.createDataFrame([], schema=my_schema_elig)\n",
    "    leftover_segment = {}\n",
    "    for segmentations in segmentation_name:\n",
    "        try:\n",
    "            segment = tox_con.segmentation(segmentations)\n",
    "            frontend_name = segment.frontend_name\n",
    "            propensities = segment.propensities\n",
    "            final_propensity = \"\".join(propensities)\n",
    "            tag = segment.tags\n",
    "            final_tag = \"\".join(tag)\n",
    "            type_of_segment = segment.segment_type\n",
    "            percentile_of_segment = segment.type\n",
    "            rec = spark.read.csv(latest_file, header=True)\n",
    "            path = rec.filter(f.col('audience')== segmentations).select('filepath').distinct().collect()[0][0]\n",
    "            period = latest_file[-12:-4]\n",
    "            segment_file = spark.read.format(\"delta\").load(path)\n",
    "\n",
    "            segment_file = (\n",
    "                segment_file.withColumn(\"SEGMENTATION\", f.lit(segmentations))\n",
    "                .withColumn('FRONTEND_NAME', f.lit(frontend_name))\n",
    "                .withColumn(\"PROPENSITY\", f.lit(final_propensity))\n",
    "                .withColumn(\"SEGMENT_TYPE\", f.lit(type_of_segment))\n",
    "                .withColumn(\"PERCENTILE_SEGMENT\", f.lit(percentile_of_segment))\n",
    "                .withColumn(\"PERIOD\", f.lit(period))\n",
    "                .withColumn(\"TAGS\", f.lit(final_tag))\n",
    "            \n",
    "            )\n",
    "            \n",
    "            segment_file = segment_file.filter(f.col(\"SEGMENT\").isin(segment.propensities))\n",
    "            segment_file = segment_file.select(\"EHHN\", \"SEGMENT\", \"SEGMENTATION\", \"FRONTEND_NAME\",\n",
    "                                                \"PROPENSITY\", \"SEGMENT_TYPE\", \"PERCENTILE_SEGMENT\",\"PERIOD\",\"TAGS\")\n",
    "            df = df.union(segment_file)\n",
    "            eligibility_analytical_dataset = df.join(latest_eligibility_df, on=\"ehhn\", how='left')\n",
    "\n",
    "            ### overall logic:\n",
    "\n",
    "            column_names = ['NATIVE_ELIGIBLE_FLAG','TDC_ELIGIBLE_FLAG','SSE_ELIGIBLE_FLAG','FFD_ELIGIBLE_FLAG','EMAIL_ELIGIBLE_FLAG','PUSH_FLAG',\n",
    "                            'FACEBOOK_FLAG','PANDORA_FLAG','CHICORY_FLAG','PUSH_FLAG','PREROLL_VIDEO_ELIGIBLE_FLAG','PINTEREST_ELIGIBLE_FLAG','ROKU_FLAG']\n",
    "\n",
    "            # Register UDF\n",
    "            contains_Y_udf = udf(any_column_contains_Y, BooleanType())\n",
    "\n",
    "            # Apply UDF to create a new column\n",
    "\n",
    "\n",
    "            eligibility_with_on_off_overall_flag = eligibility_analytical_dataset.withColumn('ONSITE_COUNT', f.when(((f.col('TDC_ELIGIBLE_FLAG') == 'Y') | (f.col('SSE_ELIGIBLE_FLAG') == 'Y') | (f.col('FFD_ELIGIBLE_FLAG') == 'Y') | (f.col('SS_ELIGIBLE_FLAG') == 'Y') | (f.col('PUSH_FLAG') == 'Y')), '1').otherwise('0').cast('integer'))\\\n",
    "                            .withColumn('OFFSITE_COUNT', f.when(((f.col('FACEBOOK_FLAG') == 'Y') | (f.col('LIVERAMP_FLAG') == 'Y') | (f.col('PANDORA_FLAG') == 'Y') | (f.col('PINTEREST_ELIGIBLE_FLAG') =='Y') | (f.col('PREROLL_VIDEO_ELIGIBLE_FLAG') == 'Y') | (f.col('CBA_ELIGIBLE_FLAG') == 'Y')), '1').otherwise('0').cast('integer'))\\\n",
    "                            .withColumn(\"OVERALL_ELIGIBILITY\", contains_Y_udf(*[col(column) for column in column_names]))\n",
    "\n",
    "            secondary_page_table = (eligibility_with_on_off_overall_flag.select(\"EHHN\",\"SEGMENT\",\"SEGMENTATION\",\"FRONTEND_NAME\",\"PROPENSITY\",\"SEGMENT_TYPE\",\"PERCENTILE_SEGMENT\",\"PERIOD\",\"TAGS\",\"ONSITE_COUNT\",\"OFFSITE_COUNT\",\"OVERALL_ELIGIBILITY\"))                           \n",
    "        \n",
    "        except Exception as e:\n",
    "                leftover_segment[segmentations] = (str(e) + str(dir_contents))\n",
    "                pass\n",
    "\n",
    "    return secondary_page_table, leftover_segment\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e725243c-15e7-4a8d-a56a-b009960a10c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_elig_total_hh(combined_segmentation_df):\n",
    "  segmentation_count_df = (combined_segmentation_df.groupBy('SEGMENTATION', 'PERIOD')\n",
    "                        .agg(f.count('EHHN').alias(\"TOTAL_HH\"),\n",
    "                        f.max(\"frontend_name\").alias(\"FRONTEND_NAME\"),\n",
    "                        f.max(\"SEGMENT\").alias(\"SEGMENT\"),\n",
    "                        f.max(\"TAGS\").alias(\"TAGS\"),\n",
    "                        f.max(\"PROPENSITY\").alias(\"PROPENSITY\"),\n",
    "                        f.max(\"SEGMENT_TYPE\").alias(\"SEGMENT_TYPE\"),\n",
    "                        f.max(\"PERCENTILE_SEGMENT\").alias(\"PERCENTILE_SEGMENT\"),\n",
    "                        )\n",
    "                        .select(\"SEGMENTATION\",\"PERIOD\",\"TOTAL_HH\",\"FRONTEND_NAME\",\"SEGMENT\",\"TAGS\",\"PROPENSITY\",\"SEGMENT_TYPE\",\"PERCENTILE_SEGMENT\")\n",
    "                        \n",
    "                  )\n",
    "                  \n",
    "  segmentation_onsite_offsite_count = (combined_segmentation_df.groupBy('SEGMENTATION', 'PERIOD')\n",
    "                          .agg(f.count('EHHN').alias(\"TOTAL_HH\"),\n",
    "                          f.max(\"frontend_name\").alias(\"FRONTEND_NAME\"),\n",
    "                          f.max(\"SEGMENT\").alias(\"SEGMENT\"),\n",
    "                          f.max(\"PROPENSITY\").alias(\"PROPENSITY\"),\n",
    "                          f.max(\"SEGMENT_TYPE\").alias(\"SEGMENT_TYPE\"),\n",
    "                          f.max(\"PERCENTILE_SEGMENT\").alias(\"PERCENTILE_SEGMENT\"),\n",
    "                          f.sum('ONSITE_COUNT').alias('ONSITE_COUNTS'),\n",
    "                          f.sum('OFFSITE_COUNT').alias('OFFSITE_COUNTS'),\n",
    "                          f.max(\"TAGS\").alias(\"TAGS\")\n",
    "                          )\n",
    "                          .select(\"SEGMENTATION\", \"PERIOD\", \"TOTAL_HH\", \"FRONTEND_NAME\", \"SEGMENT\", \"PROPENSITY\", \"SEGMENT_TYPE\", \"PERCENTILE_SEGMENT\", \"ONSITE_COUNTS\", \"OFFSITE_COUNTS\", \"TAGS\")\n",
    "                          \n",
    "                      )\n",
    "  \n",
    "  return segmentation_count_df, segmentation_onsite_offsite_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b983866-358c-436c-9f65-347f6ca127f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def writing_func(segmentation_count_df, segmentation_onsite_offsite_count):\n",
    "    period = segmentation_count_df.select(\"PERIOD\").collect()[0][0]\n",
    "    segmentation_count_df.coalesce(20).write.mode('overwrite').parquet(f\"{hh_location}/PERIOD={period}\")\n",
    "    segmentation_onsite_offsite_count.coalesce(20).write.mode('overwrite').parquet(f\"{elig_file}/PERIOD={period}\")\n",
    "\n",
    "    return \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c7ff52-b941-4c2d-88d5-46d1d8e6d084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "for i in dir_contents:\n",
    "  if len(dir_contents) >= 1:\n",
    "      try:\n",
    "          sc_pg, leftover_segment = get_periodic_hh(i)\n",
    "          print(leftover_segment)\n",
    "          sg_df, on_off_df = get_elig_total_hh(sc_pg)\n",
    "          writing_func(sg_df, on_off_df)\n",
    "          print(f\"done for {i}\")\n",
    "\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03ac974d-afe9-414f-a584-47fd33c1cf41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if len(dir_contents) >= 1:\n",
    "    display(sc_pg.limit(100))\n",
    "    display(on_off_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c335700-74a8-45a6-adcf-159896733165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import resources.config as res_config\n",
    "from functools import reduce\n",
    "from effodata import ACDS, golden_rules, Joiner, Sifter, Equality, join_on\n",
    "from kpi_metrics import KPI, AliasMetric, CustomMetric, AliasGroupby\n",
    "from IPython.display import FileLink \n",
    "import sys\n",
    "from typing import Union\n",
    "import datetime as dt\n",
    "from datetime import timedelta, datetime, date\n",
    "from PIL import Image\n",
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window\n",
    "import commodity_segmentations.config as con\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import resources.config as config\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "segmentation_list = get_all_segmentation_name()\n",
    "acds = ACDS(use_sample_mart = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b6db633-0a95-45f7-812f-659851e09f08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hh_count_df = spark.read.parquet(f\"{hh_location}\").filter(f.col(\"TAGS\").isin('Alcohol'))\n",
    "alc_segment = hh_count_df.select(\"SEGMENTATION\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "valid_image = spark.read.parquet(image_location)\n",
    "\n",
    "srt_dt = (date.today() - timedelta(days=385)).strftime(\"%Y%m%d\")\n",
    "end_dt = (date.today() - timedelta(days=10)).strftime(\"%Y%m%d\")\n",
    "srt_dt, end_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79a898fb-5dd6-4f65-81e3-ec77e20c51cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "my_schema_elig = t.StructType([\n",
    "    t.StructField(\"UPC\", t.StringType(), True),\n",
    "    t.StructField(\"URL\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENTATION\", t.StringType(), True),\n",
    "])\n",
    "final_df = spark.createDataFrame([], schema=my_schema_elig)\n",
    "failed_seg = {}\n",
    "\n",
    "transaction = acds.get_transactions(start_date=srt_dt, end_date=end_dt)\n",
    "gtins = (transaction.groupBy('gtin_no')\n",
    "        .agg(f.count('transaction_code').alias('Total Redemers'),\n",
    "            f.count('ehhn').alias(\"Total Households\"),\n",
    "            f.countDistinct('ehhn').alias(\"Total Distinct Counts\"))\n",
    "        .sort('Total Redemers', ascending=False)\n",
    "        .cache())\n",
    "\n",
    "for i in segmentation_list: \n",
    "        try: \n",
    "            if i not in alc_segment:\n",
    "                seg_dir = f\"{con.output_fp}upc_lists/{i}/\"\n",
    "                dir_contents = dbutils.fs.ls(seg_dir)\n",
    "                dir_contents = [x[0] for x in dir_contents if i in x[1]]\n",
    "                dir_contents.sort()\n",
    "                seg_fp = dir_contents[-1]\n",
    "                py_df = spark.read.format(\"delta\").load(seg_fp)\n",
    "            else:\n",
    "                pim_fp = res_config.get_latest_modified_directory(res_config.azure_pim_core_by_cycle)\n",
    "                pim = res_config.spark.read.parquet(pim_fp)\n",
    "                pim = pim.select(\n",
    "                f.col(\"upc\").alias('upc_no'),\n",
    "                f.col('familyTree.commodity').alias('commodity'),\n",
    "                f.col('familyTree.subCommodity').alias('subcommodity')\n",
    "                )\n",
    "\n",
    "                pim = (pim.withColumn(\"commodity\", f.col('commodity').getItem('name'))\n",
    "                    .withColumn('subcommodity', f.col('subcommodity').getItem('name'))\n",
    "                    .na.drop(subset=['subcommodity'])\n",
    "                )\n",
    "                alco_dict = tox_con.ss_dict[i]\n",
    "                alc_comes = alco_dict[\"commodities\"]\n",
    "                alc_sucomes = alco_dict[\"sub_commodities\"]\n",
    "                py_df = pim.filter(f.col(\"commodity\").isin(alc_comes) | f.col(\"subcommodity\").isin(alc_sucomes)).select('upc_no').withColumnRenamed('upc_no','gtin_no'). dropDuplicates()\n",
    "\n",
    "            getting_joined = py_df.join(gtins, on='gtin_no', how='left').sort('Total Redemers', ascending=False)\n",
    "            df = getting_joined.join(valid_image, on='gtin_no', how='inner').sort('Total Redemers', ascending=False).select('gtin_no','URL').limit(10)\n",
    "            upc_file = (df.withColumn(\"SEGMENTATION\", f.lit(i)))\n",
    "            final_df = final_df.union(upc_file)\n",
    "\n",
    "        except Exception as e:\n",
    "            failed_seg[i] = str(e)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09e3742d-4cd4-4a6f-a69d-e474dcfffc92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.write.partitionBy('SEGMENTATION').mode(\"overwrite\").parquet(f\"{image_upc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40de543f-f64e-4176-ae04-0450d7677a93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pim_fp = res_config.get_latest_modified_directory(res_config.azure_pim_core_by_cycle)\n",
    "# pim = res_config.spark.read.parquet(pim_fp)\n",
    "# pim = pim.select(\n",
    "#   f.col(\"upc\").alias('upc_no'),\n",
    "#   f.col('familyTree.commodity').alias('commodity'),\n",
    "#   f.col('familyTree.subCommodity').alias('subcommodity')\n",
    "# )\n",
    "\n",
    "# pim = (pim.withColumn(\"commodity\", f.col('commodity').getItem('name'))\n",
    "#        .withColumn('subcommodity', f.col('subcommodity').getItem('name'))\n",
    "#        .na.drop(subset=['subcommodity'])\n",
    "# )\n",
    "\n",
    "# df = spark.read.parquet(\"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/audience_factory/powerbi_inputs/household_counts/\").filter(f.col(\"TAGS\").isin('Alcohol'))\n",
    "# alc_segment = df.select(\"SEGMENTATION\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# alco_dict = con.ss_dict[alc_segment[0]]\n",
    "# alc_comes = alco_dict[\"commodities\"]\n",
    "# alc_sucomes = alco_dict[\"sub_commodities\"]\n",
    "\n",
    "# upcs = pim.filter(f.col(\"commodity\").isin(alc_comes) | f.col(\"subcommodity\").isin(alc_sucomes)).select('upc_no').withColumnRenamed('upc_no','gtin_no').dropDuplicates()\n",
    "# # alc_upcs = upcs.select(\"upc_no\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# transaction = acds.get_transactions(start_date=srt_dt, end_date=end_dt)\n",
    "# gtins = (transaction.groupBy('gtin_no')\n",
    "#         .agg(f.count('transaction_code').alias('Total Redemers'),\n",
    "#             f.count('ehhn').alias(\"Total Households\"),\n",
    "#             f.countDistinct('ehhn').alias(\"Total Distinct Counts\"))\n",
    "#         .sort('Total Redemers', ascending=False))\n",
    "# getting_joined = upcs.join(gtins, on='gtin_no', how='left').sort('Total Redemers', ascending=False).select('gtin_no')\n",
    "# n = 10\n",
    "# image_upcs, upc_url = draw_nonempty_upcs(getting_joined, n)\n",
    "\n",
    "# # Combine the lists using zip\n",
    "# combined_data = zip(image_upcs, upc_url)\n",
    "\n",
    "# # Create a DataFrame with column names\n",
    "# df = spark.createDataFrame(combined_data, [\"UPC\", \"URL\"])\n",
    "# upc_file = (df.withColumn(\"SEGMENTATION\", f.lit(alc_segment[0])))\n",
    "\n",
    "# # final_df = final_df.union(upc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca71f43f-46f7-431a-9cdc-af92965b6f16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d69efd-0ff0-41ec-a8ba-32d7a58d8943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Config where segmentations/segmentation class is stored\n",
    "import toolbox.config as con\n",
    "#importing required libraries\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.types as t\n",
    "\n",
    "segmentation_name = con.segmentations.all_segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8789383c-c56d-4692-bb7a-4e3f17162e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_schema = t.StructType([\n",
    "    t.StructField(\"EHHN\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENT\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENTATION\", t.StringType(), True),\n",
    "    t.StructField(\"FRONTEND_NAME\", t.StringType(), True),\n",
    "    t.StructField(\"PROPENSITY\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENT_TYPE\", t.StringType(), True),\n",
    "    t.StructField(\"PERCENTILE_SEGMENT\", t.StringType(), True),\n",
    "    t.StructField(\"PERIOD\", t.StringType(), True),\n",
    "        t.StructField(\"TAGS\", t.StringType(), True)\n",
    "    ])\n",
    "df = spark.createDataFrame([], schema=my_schema)\n",
    "\n",
    "my_schema = t.StructType([\n",
    "    t.StructField(\"EHHN\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENT\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENTATION\", t.StringType(), True),\n",
    "    t.StructField(\"FRONTEND_NAME\", t.StringType(), True),\n",
    "    t.StructField(\"PROPENSITY\", t.StringType(), True),\n",
    "    t.StructField(\"SEGMENT_TYPE\", t.StringType(), True),\n",
    "    t.StructField(\"PERCENTILE_SEGMENT\", t.StringType(), True),\n",
    "    t.StructField(\"PERIOD\", t.StringType(), True),\n",
    "        t.StructField(\"TAGS\", t.StringType(), True)\n",
    "    ])\n",
    "df_2 = spark.createDataFrame([], schema=my_schema)\n",
    "stratum_week = \"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/audience_factory/powerbi_inputs/stratum_week/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f54c991-e742-4458-ae26-c5c21663661d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for segmentation in segmentation_name:\n",
    "  filtered_time_period_lst = spark.read.parquet(stratum_week).filter(f.col('SEGMENTATION').isin(segmentation)).select(\"PERIOD\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "  final_per_list = ', '.join(map(str, filtered_time_period_lst))\n",
    "  try:\n",
    "      segment = con.segmentation(segmentation)\n",
    "      latest_file = segment.files\n",
    "      for i in latest_file:\n",
    "          if i[-9:-1] not in final_per_list:\n",
    "              frontend_name = segment.frontend_name\n",
    "              propensities = segment.propensities\n",
    "              final_propensity = \"\".join(propensities)\n",
    "              tag = segment.tags\n",
    "              final_tag = \"\".join(tag)\n",
    "              type_of_segment = segment.segment_type\n",
    "              percentile_of_segment = segment.type\n",
    "              directory = segment.directory\n",
    "              period = i[-9:-1]\n",
    "              path = directory+i\n",
    "              segment_file = spark.read.format(\"delta\").load(path)\n",
    "\n",
    "              segment_file = (\n",
    "                  segment_file.withColumn(\"SEGMENTATION\", f.lit(segmentation))\n",
    "                  .withColumn('FRONTEND_NAME', f.lit(frontend_name))\n",
    "                  .withColumn(\"PROPENSITY\", f.lit(final_propensity))\n",
    "                  .withColumn(\"SEGMENT_TYPE\", f.lit(type_of_segment))\n",
    "                  .withColumn(\"PERCENTILE_SEGMENT\", f.lit(percentile_of_segment))\n",
    "                  .withColumn(\"PERIOD\", f.lit(period))\n",
    "                  .withColumn(\"TAGS\", f.lit(final_tag))\n",
    "\n",
    "              )\n",
    "\n",
    "              segment_file = segment_file.filter(f.col(\"SEGMENT\").isin(segment.propensities))\n",
    "              segment_file = segment_file.select(\"EHHN\", \"SEGMENT\", \"SEGMENTATION\", \"FRONTEND_NAME\",\n",
    "                                                  \"PROPENSITY\", \"SEGMENT_TYPE\", \"PERCENTILE_SEGMENT\",\"PERIOD\",\"TAGS\")\n",
    "              df = df.union(segment_file)\n",
    "\n",
    "      df_2 = df.groupBy('SEGMENTATION', 'PERIOD').agg(f.count('EHHN').alias(\"TOTAL_HH\"),\n",
    "                            f.max(\"frontend_name\").alias(\"FRONTEND_NAME\"),\n",
    "                            f.max(\"SEGMENT\").alias(\"SEGMENT\"),\n",
    "                            f.max(\"PROPENSITY\").alias(\"PROPENSITY\"),\n",
    "                            f.max(\"SEGMENT_TYPE\").alias(\"SEGMENT_TYPE\"),\n",
    "                            f.max(\"PERCENTILE_SEGMENT\").alias(\"PERCENTILE_SEGMENT\"),\n",
    "                            f.max(\"TAGS\").alias(\"TAGS\")\n",
    "                            ).select(\"SEGMENTATION\", \"PERIOD\", \"TOTAL_HH\", \"FRONTEND_NAME\", \"SEGMENT\", \"PROPENSITY\", \"SEGMENT_TYPE\", \"PERCENTILE_SEGMENT\",\"TAGS\")\n",
    "\n",
    "      df_2.coalesce(20) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy('PERIOD') \\\n",
    "        .parquet(f\"{stratum_week}/SEGMENTATION={segmentation}\")\n",
    "\n",
    "  except Exception as e:\n",
    "    print(f\"not possible for {segmentation}\")\n",
    "    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2b3abed-b94a-4017-82b2-a7545f3eac99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.parquet(f\"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/audience_factory/powerbi_inputs/stratum_week/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478349b0-bda9-4af5-81f8-926968776074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for audience in con.segmentations.all_segmentations:\n",
    "  #Create the segment class for vegetarian\n",
    "  segment = con.segmentation(audience)\n",
    "  #Directory where upc lists are stored\n",
    "  upc_dir = segment.upc_directory\n",
    "  #Pull the filename for the latest UPC list. For audiences that are UPC based, most of their\n",
    "  #UPC lists are refreshed on a weekly basis and stored away\n",
    "  upc_fn = segment.upc_files[-1]\n",
    "  #Create the filepath for where the UPC list is stored\n",
    "  upc_fp = upc_dir + upc_fn\n",
    "  #The UPC lists are stored as delta files\n",
    "  upc_df = spark.read.format(\"delta\").load(upc_fp)\n",
    "  upc_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68eb8e1f-be67-4e5f-bae8-d11af68ef84b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "content_management_wns",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
