{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ccd778e-6226-4b61-9d37-0b0e7dccd743",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Importing All the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kayday as kd\n",
    "import seg\n",
    "import toolbox.config as con\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from effodata import ACDS, golden_rules, Sifter, Equality, Joiner\n",
    "from toolbox.config import segmentation\n",
    "from kpi_metrics import KPI, AliasMetric, CustomMetric, AliasGroupby, available_metrics, get_metrics\n",
    "from seg.utils import DateType\n",
    "from flowcate.files import FilePath\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import BooleanType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d58ec7-9d45-41de-94ba-d41dd4c6d4b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Getting Eligiblity Table\n",
    "\n",
    "eligibility_path = FilePath('abfss://landingzone@sa8451entlakegrnprd.dfs.core.windows.net/mart/comms/prd/fact/eligibility_fact')\n",
    "latest_eligibility_df = spark.read.parquet(eligibility_path.find_latest_file()) #This will give us the Dates we are using in.\n",
    "print(eligibility_path.find_latest_file())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b74136d0-58d7-42c0-88c9-ad0c91037f09",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Getting all the Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d514d0-dd4f-43ba-a9ff-0182e43b0f58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_all_segmentation_name():\n",
    "    '''\n",
    "    Get all the Segmentation names\n",
    "    \n",
    "    '''\n",
    "    segmentation_names = con.segmentations.all_segmentations\n",
    "\n",
    "    return segmentation_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37eb7bce-264f-4bc5-be24-68474163bb43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Combining All Segmentations Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3adb74dc-97c0-4ba8-aff1-68abb0833ba5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_all_columns(segmentation_names):\n",
    "    '''\n",
    "    Input: Segmentation names\n",
    "    Output: Final Dataframe with all segmentation\n",
    "    '''\n",
    "\n",
    "    my_schema = t.StructType([\n",
    "        t.StructField(\"EHHN\", t.StringType(), True),\n",
    "        t.StructField(\"SEGMENT\", t.StringType(), True),\n",
    "        t.StructField(\"SEGMENTATION\", t.StringType(), True),\n",
    "        t.StructField(\"FRONTEND_NAME\", t.StringType(), True),\n",
    "        t.StructField(\"PROPENSITY\", t.StringType(), True),\n",
    "        t.StructField(\"SEGMENT_TYPE\", t.StringType(), True),\n",
    "        t.StructField(\"PERCENTILE_SEGMENT\", t.StringType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame([], schema=my_schema)\n",
    "    leftover_segment = []\n",
    "    for segmentations in segmentation_names:\n",
    "          try:\n",
    "            segment = con.segmentation(segmentations)\n",
    "            frontend_name = segment.frontend_name\n",
    "            propensities = segment.propensities\n",
    "            final_propensity = \"\".join(propensities)\n",
    "            type_of_segment = segment.segment_type\n",
    "            percentile_of_segment = segment.type\n",
    "            latest_file = segment.files[-1]\n",
    "            reading_file = segment.directory + latest_file\n",
    "            segment_file = spark.read.format(\"delta\").load(reading_file)\n",
    "\n",
    "            segment_file = (\n",
    "                  segment_file.withColumn(\"SEGMENTATION\", f.lit(segmentations))\n",
    "                  .withColumn('FRONTEND_NAME', f.lit(frontend_name))\n",
    "                  .withColumn(\"PROPENSITY\", f.lit(final_propensity))\n",
    "                  .withColumn(\"SEGMENT_TYPE\", f.lit(type_of_segment))\n",
    "                  .withColumn(\"PERCENTILE_SEGMENT\", f.lit(percentile_of_segment))\n",
    "            )\n",
    "            \n",
    "            segment_file = segment_file.filter(f.col(\"SEGMENT\").isin(segment.propensities))\n",
    "            segment_file = segment_file.select(\"EHHN\", \"SEGMENT\", \"SEGMENTATION\", \"FRONTEND_NAME\",\n",
    "                                                \"PROPENSITY\", \"SEGMENT_TYPE\", \"PERCENTILE_SEGMENT\")\n",
    "            df = df.union(segment_file)\n",
    "          \n",
    "          except Exception as e:\n",
    "                leftover_segment.append(str(e))\n",
    "                pass\n",
    "\n",
    "    return df, leftover_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e20e65c8-a7a5-4db8-af67-bf8b54016df4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "segmentation_name = get_all_segmentation_name()\n",
    "final_df, leftover_seg = get_all_columns(segmentation_names=segmentation_name)\n",
    "segmentation_name_df = spark.createDataFrame([row for row in [(value,) for value in segmentation_name]], [\"SEGMENTATION\"])\n",
    "\n",
    "#If the result is empty then we are getting all the segmentation if not then we are missing one.\n",
    "\n",
    "display(segmentation_name_df.join(final_df, on='SEGMENTATION', how='left_anti')\n",
    "        .select('segmentation').distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92fd72a3-12e2-419e-a46c-aa2fa54d78df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_df.sort('EHHN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efec4d8a-8a1b-4f36-86ad-7720aab888d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eligibility_analytical_dataset = final_df.join(latest_eligibility_df, on=\"ehhn\", how='left')\n",
    "\n",
    "### overall logic:\n",
    "\n",
    "column_names = ['NATIVE_ELIGIBLE_FLAG','TDC_ELIGIBLE_FLAG','SSE_ELIGIBLE_FLAG','FFD_ELIGIBLE_FLAG','EMAIL_ELIGIBLE_FLAG','PUSH_FLAG',\n",
    "                'FACEBOOK_FLAG','PANDORA_FLAG','CHICORY_FLAG','PUSH_FLAG','PREROLL_VIDEO_ELIGIBLE_FLAG','PINTEREST_ELIGIBLE_FLAG','ROKU_FLAG']\n",
    "\n",
    "# Function to check if any column contains 'Y'\n",
    "def any_column_contains_Y(*args):\n",
    "    return 'Y' in args\n",
    "\n",
    "# Register UDF\n",
    "contains_Y_udf = udf(any_column_contains_Y, BooleanType())\n",
    "\n",
    "# Apply UDF to create a new column\n",
    "\n",
    "\n",
    "eligibility_with_on_off_overall_flag = eligibility_analytical_dataset.withColumn('onsite_flag', f.when(((f.col('TDC_ELIGIBLE_FLAG') == 'Y') | (f.col('SSE_ELIGIBLE_FLAG') == 'Y') | (f.col('FFD_ELIGIBLE_FLAG') == 'Y') | (f.col('SS_ELIGIBLE_FLAG') == 'Y') | (f.col('PUSH_FLAG') == 'Y')), '1').otherwise('0').cast('integer'))\\\n",
    "                  .withColumn('offsite_flag', f.when(((f.col('FACEBOOK_FLAG') == 'Y') | (f.col('LIVERAMP_FLAG') == 'Y') | (f.col('PANDORA_FLAG') == 'Y') | (f.col('PINTEREST_ELIGIBLE_FLAG') =='Y') | (f.col('PREROLL_VIDEO_ELIGIBLE_FLAG') == 'Y') | (f.col('CBA_ELIGIBLE_FLAG') == 'Y')), '1').otherwise('0').cast('integer'))\\\n",
    "                  .withColumn(\"Overall Eligibility\", contains_Y_udf(*[col(column) for column in column_names]))\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b5085de-de14-42b1-b26e-be8f4f52baba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(eligibility_with_on_off_overall_flag.limit(5)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8507bf2-1808-46bd-9f93-29235e926b88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "primary_page_table = (final_df.groupBy('SEGMENTATION')\n",
    "                        .agg(f.count('EHHN').alias(\"TOTAL_HH\"),\n",
    "                        f.max(\"frontend_name\").alias(\"FRONTEND_NAME\"),\n",
    "                        f.max(\"SEGMENT\").alias(\"SEGMENT\"),\n",
    "                        f.max(\"PROPENSITY\").alias(\"PROPENSITY\"),\n",
    "                        f.max(\"SEGMENT_TYPE\").alias(\"SEGMENT_TYPE\"),\n",
    "                        f.max(\"PERCENTILE_SEGMENT\").alias(\"PERCENTILE_SEGMENT\"),\n",
    "                        ))\n",
    "                  \n",
    "secondary_page_table = (eligibility_with_on_off_overall_flag.groupBy('SEGMENTATION')\n",
    "                        .agg(f.count('EHHN').alias(\"TOTAL_HH\"),\n",
    "                        f.max(\"frontend_name\").alias(\"FRONTEND_NAME\"),\n",
    "                        f.max(\"SEGMENT\").alias(\"SEGMENT\"),\n",
    "                        f.max(\"PROPENSITY\").alias(\"PROPENSITY\"),\n",
    "                        f.max(\"SEGMENT_TYPE\").alias(\"SEGMENT_TYPE\"),\n",
    "                        f.max(\"PERCENTILE_SEGMENT\").alias(\"PERCENTILE_SEGMENT\"),\n",
    "                        f.sum('onsite_flag').alias('ONSITE_COUNT'),\n",
    "                        f.sum('offsite_flag').alias('OFFSITE_COUNT')\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "773196d8-76a9-4c49-b22d-6457e13ca2f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(secondary_page_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e300ecec-941f-4263-a59f-29c97c5624f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "primary_page_table.write.mode('overwrite').parquet(f\"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/audience_factory/powerbi_inputs/household_counts/\")\n",
    "secondary_page_table.write.mode('overwrite').parquet(f\"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/audience_factory/powerbi_inputs/eligibility_counts/\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "content_management_wns",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
