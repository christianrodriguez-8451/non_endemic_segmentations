{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c06e6c9-f6c4-44e8-b503-819dc8c10a85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install databricks-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51f268bd-10c4-4ad1-87f5-5080c1fd9bd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DATABRICKS_HOST=\"https://adb-291758323461480.0.azuredatabricks.net\"\n",
    "DATABRICKS_TOKEN=\"dapia2925902096f9b874fd33f4236a1b0b5-2\"\n",
    "\n",
    "import os\n",
    "from databricks_cli.sdk.api_client import ApiClient\n",
    "from databricks_cli.jobs.api import JobsApi\n",
    "from databricks_cli.runs.api import RunsApi\n",
    "\n",
    "# Authenticate, assuming you have credentials stored in environment variables\n",
    "api_client = ApiClient(\n",
    "host = DATABRICKS_HOST,\n",
    "token = DATABRICKS_TOKEN\n",
    ")\n",
    "\n",
    "# Initialize the Jobs API\n",
    "jobs_api = JobsApi(api_client)\n",
    "run_api = RunsApi(api_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873c58e5-7a28-421a-a4d7-53c7c493b693",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math as mt\n",
    "import time as tm\n",
    "\n",
    "#Get all segments I need to push through the API\n",
    "fp = \"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/commodity_segments/output/cycle_date=2023-07-20/\"\n",
    "segs = dbutils.fs.ls(fp)\n",
    "segs = [x[0] for x in segs]\n",
    "segs += [\"abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/commodity_segments/output/cycle_date=2023-07-20/organic/\"]\n",
    "segs = [x for x in segs if not \"valentines\" in x]\n",
    "segs.sort()\n",
    "segs.reverse()\n",
    "\n",
    "job_id = 216755294515781\n",
    "\n",
    "#Max concurrent runs will determine how many we can run in chunkks and how many rounds of chunks we need to do\n",
    "runs_max = 5\n",
    "#Minutes to wait before checking active runs again\n",
    "mins = 10\n",
    "for s in segs:\n",
    "  #Wait until all active runs are finished before you execute the next one\n",
    "  runs = run_api.list_runs(job_id, True, False, 0, 1000)\n",
    "  runs = runs[\"runs\"]\n",
    "  active_runs = [x for x in runs if x[\"state\"][\"life_cycle_state\"] == \"RUNNING\"]\n",
    "  while len(active_runs) == runs_max:\n",
    "    print(\"Max jobs are still running ...\")\n",
    "    tm.sleep(60*mins)\n",
    "\n",
    "    runs = run_api.list_runs(job_id, True, False, 0, 1000)\n",
    "    runs = runs[\"runs\"]\n",
    "    active_runs = [x for x in runs if x[\"state\"][\"life_cycle_state\"] == \"RUNNING\"]\n",
    "    \n",
    "  jobs_api.run_now(job_id, jar_params=[\"\"], notebook_params={\"upc_list_path_api\": s}, python_params=[\"\"], spark_submit_params=[\"\"])\n",
    "  print(\"Executed API vintage job for {}!\\n\".format(s))\n",
    "  tm.sleep(60*0.25)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9df92db-6e62-4d9f-a667-4672d012d3d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "job_id = 216755294515781\n",
    "s = 'abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/commodity_segments/output/cycle_date=2023-07-20/unleaded_plus_car_owners/'\n",
    "jobs_api.run_now(job_id, jar_params=[\"\"], notebook_params={\"upc_list_path_api\": s}, python_params=[\"\"], spark_submit_params=[\"\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a065f508-f8c4-4a3d-83fb-016174b66046",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Call Create_Query_Vintages_API",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
