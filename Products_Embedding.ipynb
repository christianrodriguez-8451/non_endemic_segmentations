{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c590186d-611a-47b2-b6f1-d45769c4448a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Define service principals\n",
    "service_credential = dbutils.secrets.get(scope='kv-8451-tm-media-dev',key='spTmMediaDev-pw')\n",
    "service_application_id = dbutils.secrets.get(scope='kv-8451-tm-media-dev',key='spTmMediaDev-app-id')\n",
    "directory_id = \"5f9dc6bd-f38a-454a-864c-c803691193c5\"\n",
    "storage_account = 'sa8451entlakegrnprd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48225d4f-dfce-4c0e-81da-80df88d45a28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Set configurations\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", service_application_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "694bdc74-3b07-469c-b5d5-2995cfe95434",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Define service principals\n",
    "service_credential = dbutils.secrets.get(scope='kv-8451-tm-media-dev',key='spTmMediaDev-pw')\n",
    "service_application_id = dbutils.secrets.get(scope='kv-8451-tm-media-dev',key='spTmMediaDev-app-id')\n",
    "directory_id = \"5f9dc6bd-f38a-454a-864c-c803691193c5\"\n",
    "storage_account = 'sa8451dbxadhocprd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71df4379-59c0-41d3-89c9-2beeae1a7e0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Set configurations\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", service_application_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b480bcb-08e4-4262-b0a3-956c3fa499a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Define service principals\n",
    "service_credential = dbutils.secrets.get(scope='kv-8451-tm-media-dev',key='spTmMediaDev-pw')\n",
    "service_application_id = dbutils.secrets.get(scope='kv-8451-tm-media-dev',key='spTmMediaDev-app-id')\n",
    "directory_id = \"5f9dc6bd-f38a-454a-864c-c803691193c5\"\n",
    "storage_account = 'sa8451posprd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daa70c1a-5d67-44cf-8a35-34e1922364ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Set configurations\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", service_application_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e51767-33db-4efb-9b27-5f7d0588a762",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "from effodata import ACDS, golden_rules, Joiner, Sifter, Equality, sifter, join_on, joiner\n",
    "from kpi_metrics import (\n",
    "     KPI,\n",
    "     AliasMetric,\n",
    "     CustomMetric,\n",
    "     AliasGroupby,\n",
    "     Rollup,\n",
    "     Cube,\n",
    "     available_metrics,\n",
    "     get_metrics\n",
    ")\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# minus 1 year\n",
    "today_year_ago = (datetime.today() - relativedelta(years=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "yesterday = (datetime.today() - relativedelta(days=1)).strftime('%Y%m%d')\n",
    "\n",
    "# SPARK SETUP\n",
    "acds = ACDS(use_sample_mart=False)\n",
    "kpi = KPI(use_sample_mart=False)\n",
    "\n",
    "# Pull acds transactions over the past year\n",
    "acds_pyear_transactions = acds.get_transactions(today_year_ago, today, apply_golden_rules=golden_rules(['customer_exclusions']))\n",
    "pyear_purchased_upcs = acds_pyear_transactions.select(acds_pyear_transactions.gtin_no).distinct()\n",
    "pyear_purchased_upcs = pyear_purchased_upcs.select(pyear_purchased_upcs.gtin_no.alias('upc'))\n",
    "\n",
    "def get_latest_modified_directory(pDirectory):\n",
    "    \"\"\"\n",
    "    get_latest_modified_file_from_directory:\n",
    "        For a given path to a directory in the data lake, return the directory that was last modified. \n",
    "        Input path format expectation: 'abfss://x@sa8451x.dfs.core.windows.net/\n",
    "    \"\"\"\n",
    "    #Set to get a list of all folders in this directory and the last modified date time of each\n",
    "    vDirectoryContentsList = list(dbutils.fs.ls(pDirectory))\n",
    "\n",
    "    #Convert the list returned from get_dir_content into a dataframe so we can manipulate the data easily. Provide it with column headings. \n",
    "    #You can alternatively sort the list by LastModifiedDateTime and get the top record as well. \n",
    "    df = spark.createDataFrame(vDirectoryContentsList,['FullFilePath', 'LastModifiedDateTime'])\n",
    "\n",
    "    #Get the latest modified date time scalar value\n",
    "    maxLatestModifiedDateTime = df.agg({\"LastModifiedDateTime\": \"max\"}).collect()[0][0]\n",
    "\n",
    "    #Filter the data frame to the record with the latest modified date time value retrieved\n",
    "    df_filtered = df.filter(df.LastModifiedDateTime == maxLatestModifiedDateTime)\n",
    "    \n",
    "    #return the file name that was last modifed in the given directory\n",
    "    return df_filtered.first()['FullFilePath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ada0426b-9751-4d8d-9889-855d8f114654",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#To do:  look for latest pinto and pim files\n",
    "pinto_path = get_latest_modified_directory('abfss://data@sa8451entlakegrnprd.dfs.core.windows.net/source/third_party/prd/pinto/')\n",
    "pinto_prods = spark.read.parquet(pinto_path)\n",
    "pim_path = get_latest_modified_directory(\"abfss://pim@sa8451posprd.dfs.core.windows.net/pim_core/by_cycle/\")\n",
    "pim_core = spark.read.parquet(pim_path)\n",
    "search_path = \"abfss://personloyalty@sa8451dbxadhocprd.dfs.core.windows.net/relevancy/e451_query2concept2vec/20221007/query2concept2vec.parquet\"\n",
    "search_emb = spark.read.parquet(search_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd560037-1b20-42a9-8517-9859d8f8c6f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import collect_set, substring_index, concat_ws, concat, split, regexp_replace, size, expr\n",
    "\n",
    "#Build sentence with upc, description and diet related meta data from PIM data source\n",
    "pim_diet = pim_core.select(f.col(\"upc_key\"), f.col(\"krogerOwnedEcommerceDescription\"), f.col(\"gtinName\"), f.col(\"diets.*\"))\n",
    "pim_diets = pim_diet.withColumn('diet', concat_ws(', ', f.col('AYERVEDIC.name'), f.col('LOW_BACTERIA.name'),\\\n",
    "     f.col('COELIAC.name'), f.col('DIABETIC.name'), f.col('FREE_FROM_GLUTEN.name'), f.col('GLYCEMIC.name'),\\\n",
    "     f.col('GRAIN_FREE.name'), f.col('HALAL.name'), f.col('HGC.name'), f.col('HIGH_PROTEIN.name'),\\\n",
    "     f.col('KEHILLA.name'), f.col('KETOGENIC.name'), f.col('KOSHER.name'), f.col('LACTOSE_FREE.name'),\\\n",
    "     f.col('LOW_CALORIE.name'), f.col('LOW_PROTEIN.name'), f.col('LOW_SALT.name'), f.col('MACROBIOTIC.name'),\\\n",
    "     f.col('METABOLIC.name'), f.col('NON_VEG.name'), f.col('PALEO.name'),\\\n",
    "     f.col('PECETARIAN.name'), f.col('PLANT_BASED.name'), f.col('RAW_FOOD.name'), f.col('VEGAN.name'),\\\n",
    "     f.col('VEGETARIAN.name'), f.col('VEG_OVO.name'), f.col('WITHOUT_BEEF.name'), f.col('WITHOUT_PORK.name')))\\\n",
    "     .withColumn('clean_commas', f.regexp_replace(f.col('diet'), ' ,', ','))\\\n",
    "     .withColumn('lowercase', f.lower(concat_ws(',', 'clean_commas')))\\\n",
    "     .withColumn('no_trailing_space', regexp_replace(f.col('lowercase'), r'\\s+$', ''))\\\n",
    "     .withColumnRenamed('no_trailing_space', 'pim_sentence')\\\n",
    "     .select('upc_key', 'krogerOwnedEcommerceDescription', 'gtinName', 'pim_sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36989adf-d863-4b5c-9a5f-d04c599b240c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Build sentence with upc, name and diet related meta data from Pinto data source\n",
    "pinto_diets = pinto_prods.select(f.explode(f.col('upcs.standard')).alias('gtin_no'), f.col('dietList'))\\\n",
    "    .withColumn(\"gtin_no\",expr(\"substring(gtin_no, 1, length(gtin_no)-1)\"))\\\n",
    "    .select(f.col('gtin_no'), f.col('dietList.slug').alias('diet_slug'))\n",
    "pinto_names = pinto_prods.select(f.explode(f.col('upcs.standard')).alias('gtin_no'), f.col('name'))\\\n",
    "    .withColumn(\"gtin_no\",expr(\"substring(gtin_no, 1, length(gtin_no)-1)\"))\\\n",
    "    .select('gtin_no', 'name').distinct()\n",
    "pinto_data = pinto_names.join(pinto_diets, 'gtin_no', 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f7ff52-83d1-4257-9ff0-e9bee89700f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Join Pinto and PIM data\n",
    "pinto_pim = pinto_data.join(pim_diets, pim_diets.upc_key == pinto_data.gtin_no, 'outer')\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "pinto_pim = pinto_pim.withColumn(\"name\", when(pinto_pim.name.isNull(), pinto_pim.krogerOwnedEcommerceDescription).otherwise(pinto_pim.name))\n",
    "\n",
    "pinto_pim = pinto_pim.withColumn(\"krogerOwnedEcommerceDescription\", when(pinto_pim.krogerOwnedEcommerceDescription.isNull(), pinto_pim.name).otherwise(pinto_pim.krogerOwnedEcommerceDescription))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5407fa02-dfb9-436c-af1a-07e6633f9618",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean data\n",
    "\n",
    "sentences = pinto_pim.withColumn('diet_string', concat_ws(',  ', f.col('diet_slug')))\\\n",
    "    .withColumn('diet_string', f.lower(concat(f.col('diet_string'))))\\\n",
    "    .withColumn('diet_sentence', split(f.col('diet_string'), ', '))\\\n",
    "    .withColumn('pimto_sentence', concat_ws(',', f.col('diet_sentence'),f.col('pim_sentence')))\\\n",
    "    .withColumn('no_leading_comma', regexp_replace(f.col('pimto_sentence'), r'^\\,+', ''))\\\n",
    "    .select('upc_key','name','gtin_no','no_leading_comma')\n",
    "sentences = sentences.select(f.coalesce(sentences[\"upc_key\"], sentences[\"gtin_no\"]).alias('gtin_no'), f.col(\"name\"),f.col('no_leading_comma').alias('diet_sentence'))\n",
    "sentences = sentences.select(f.col(\"gtin_no\"), f.col(\"name\"),split(f.col(\"diet_sentence\"),\",\").alias(\"diet_sentence_Arr\"))\n",
    "\n",
    "from pyspark.sql.functions import array_distinct\n",
    "sentences = sentences.withColumn(\"dedup_diet_sentence_Arr\", array_distinct(\"diet_sentence_Arr\"))\\\n",
    "     .withColumnRenamed('dedup_diet_sentence_Arr', 'diet_sentence')\\\n",
    "     .withColumn(\"diet_sentence\", concat_ws(\",\",f.col(\"diet_sentence\")))\\\n",
    "     .select('gtin_no', 'name', 'diet_sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb6b3152-0eb7-4862-b203-cb9808b79a28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a \"text\" field that concatenates everything you want to encode(a string that has the sentence as well as gtin)\n",
    "vector = sentences.select('diet_sentence', 'gtin_no', 'name')\\\n",
    "    .withColumn('sentence_string', concat_ws(',', f.col('diet_sentence')))\\\n",
    "    .withColumn('full_string', when(f.col('sentence_string') == '', concat(f.col('name'), f.lit('.'))).otherwise(concat(f.col('name'), f.lit(' is '), f.col('sentence_string'), f.lit('.'))))\\\n",
    "    .withColumn('diet_string', f.regexp_replace(f.col('full_string'), ',,', ','))\\\n",
    "    .select('gtin_no', 'diet_string')\n",
    "vector = vector.join(pyear_purchased_upcs, pyear_purchased_upcs.upc == vector.gtin_no)\n",
    "vector = vector.select('gtin_no', 'diet_string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d172bba-2dcc-453b-88e2-dedb67c2ed8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df = vector.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461bbf72-d709-4a40-91d9-52774f676cae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify the model directory on DBFS\n",
    "model_dir = \"/dbfs/dbfs/FileStore/users/s354840/pretrained_transformer_model\" \n",
    "\n",
    "# These packages are required for delivery\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Loading the transformer model\n",
    "model = SentenceTransformer(model_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca524f6-e899-4d61-ba04-4385b584014b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract \"text\" field into a list\n",
    "sentence = pandas_df.diet_string.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b1b602-906e-4c40-b9b0-72904ba288f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Encode vectors from sentences\n",
    "# You may need a GPU cluster to do this efficiently 3:01 p.m.\n",
    "vectors = model.encode(sentence, normalize_embeddings=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8561a301-132b-40d7-8e0f-32f2ebccdc69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas_df['vector'] = vectors.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb1b83c-52e8-4b03-81ab-d4fccae63cb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join together into output dataframe\n",
    "import pandas as pd\n",
    "output_df = spark.createDataFrame(pd.DataFrame({\"gtin_no\": pandas_df[\"gtin_no\"].to_list(), \"vector\": pandas_df[\"vector\"].to_list()}))\n",
    "output_df.write.mode(\"overwrite\").parquet('abfss://media@sa8451dbxadhocprd.dfs.core.windows.net/Users/s354840/embedded_dimensions/last_year_product_vectors_diet_description/' + today + '/upc_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86f028b5-f23f-4eae-906c-593658a25a17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Products_Embedding",
   "notebookOrigID": 775292518014095,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
